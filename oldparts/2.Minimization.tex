%\section{Covariance matrix estimation}
%\label{sec:cm_estimation}
%We define $\Corr{\bvy}[l,k]$ as the covariance matrix of the observed signal (omitting $[l,k]$ for clarity)
%\begin{equations}
%	\Corr{\bvy}
%	& = \expec{\bvy \he{\bvy}} \\
%	& = \bva \he{\bva} \var{x_1} + \bvb \he{\bvb} \var{u_1} + \Corr{\bvga}' \var{\gamma_1} + \Corr{\bvv}'\var{v_1}
%\end{equations}
%where $\sigma_{x_1}[l,k]$ is the variance of $x_1[l,k]$ (and similarly for the others), and $\Corr{\bvv}'[l,k]$ and $\Corr{\bvga}'[l,k]$ are the pseudo-normalized correlation matrices for $\bvv[l,k]$ and $\bvga[l,k]$. Now we have some assumptions:
%\begin{enumerate}
%	\item $\bva[l,k]$ is known;
%	\item $\bvv[l,k]$ is a spatially uncorrelated white noise, such that $\Corr{\bvv}' = \Id$;
%	\item $\bvga[l,k]$ is a spherically isotropic noise, and thus $\Corr{\bvga}'[l,k]$ is a purely real matrix;
%\end{enumerate}
%and all other variables are unknown. Therefore, we have 5 unknown constants in our problem: the variances for all signals at the reference sensor, and the direction of arrival (DoA) for $\bvu[l,k]$, which dictates its RTF $\bvb[l,k]$.
%
%With these assumptions we define $\Corr*{\bvy}$ as an approximation of $\Corr{\bvy}$, given by
%\begin{equations}
%	\Corr*{\bvy}
%	& \approx \Corr{\bvy} \\
%	& = \bva \he{\bva} \var*{x} + \bvb[h] \he{\bvb[h]} \var*{u} + \Gzp' \var*{\gamma} + \Id \var*{v} \\
%	& = \Corr{\bvx}' \var*{x} + \Corr{\bvu}' \var*{u} + \Corr{\bvga}' \var*{\gamma} + \Corr{\bvv}' \var*{v}
%\end{equations}
%
%It is interesting to take into consideration three properties of the (assumed/approximated) correlation matrices:
%\begin{itemize}
%	\item For the main diagonal, $\el{\Corr{\bvx}'}[i][i] = \el{\Corr{\bvu}'}[i][i] = \el{\Corr{\bvga}'}[i][i] = \el{\Corr{\bvv}'}[i][i] = 1$
%	\item Outside the main diagonal, $\el{\Id}[i][j] = 0$
%	%	\item For all values, $\Gzp'$ is a purely real matrix
%\end{itemize}

%\subsection{Cost function and minimization problem}
%
%We set our cost function as
%\begin{equation}
%	J\pts{\Corr*{\bvy}[l,k]} = \fnorm{\Corr*{\bvy}[l,k] - \Corr{\bvy}[l,k]}^2
%\end{equation}
%where $J(\Corr*{\bvy})$ is implicitly a function of $\bvb[h]$ (which is a function of $\Theta = (\theta,\phi)$, the DoA), and the variances $\var*{x}$ through $\var*{v}$. We bring back the frequency dependency for this step to emphasize that this should be done for each frequency. We denote $\el{\Corr{\bvy}}[i][j] = R_{\bvy;\bvj}$, and similarly for all other correlation matrices, with $\bvj = [i,j]$, a matrix index vector. We denote $\bvsi = [\var*{x},~\var*{u},~\var*{\gamma},~\var*{v}]$ as a vector of the unknown variances, with which we have
%\begin{equation}
%	J\pts{\Corr*{\bvy}(\bvsi,\Theta)} = \sum_{\bvj} \abs{R'_{\bvx;\bvj} \var*{x} + R'_{\bvu;\bvj}(\theta,\phi) \var*{u} + R'_{\bvga;\bvj} \var*{\gamma} + R'_{\bvv;\bvj} \var*{v} - R_{\bvy;\bvj}}^2
%\end{equation}
%which, by splitting into real and imaginary parts, gives us
%\begin{equations}
%	J\pts{\Corr*{\bvy}(\bvsi,\Theta)}
%	= \sum_{\bvj} &~ \pts{\real{\hat{R}_{\bvy;\bvj} - R_{\bvy;\bvj}}}^2 + \pts{\imag{\hat{R}_{\bvy;\bvj} - R_{\bvy;\bvj}}}^2 \\
%	= \sum_{\bvj} & \pts{\real{R'_{\bvx;\bvj}} \var*{x} + \real{R'_{\bvu;\bvj}(\theta,\phi)} \var*{u} + R'_{\bvga;\bvj} \var*{\gamma} + R'_{\bvv;\bvj} \var*{v} - \real{R_{\bvy;\bvj}}}^2 \\
%	+ & \pts{\imag{R'_{\bvx;\bvj}} \var*{x} + \imag{R'_{\bvu;\bvj}(\theta,\phi)} \var*{u} - \imag{R_{\bvy;\bvj}}}^2 
%\end{equations}
%
%By minimizing our cost function with respect to these variables, we can find the parameters that define $\Corr*{\bvy}$ which better approximate our desired matrix $\Corr{\bvy}$, from which we can then estimate our noise covariance matrix (NCM) $\Corr{\bveta}$, which is given by
%\begin{equation}
%%	\label{eq:def_NCM_Corr-bveta}
%	\Corr{\bveta} = \bvb[h] \he{\bvb[h]} \var*{u_1} + \Corr{\bvga}' \var*{\gamma_1} + \Corr{\bvv}' \var*{v}
%\end{equation}

%\subsection{Lagrangian multiplier}
%We write our Lagrangian function $\lag(\bvsi,\Theta,\bvxi,\bvmu)$ as
%\begin{equation}
%	\lag(\bvsi,\Theta,\bvxi,\bvmu) = J\pts{\Corr*{\bvy}} - \tr{\bvxi}\pts{\bvsi - \bvmu}
%\end{equation}
%with $\bvxi$ being our Lagrange multiplier vector, and $\bvmu$ our slack variable vector, as we have an inequality constraint, with $\bvmu \geq \bv{0}$. From this we write our minimization problem as
%
%We denote $\pts{\bvsi\opt,\Theta\opt,\bvxi\opt}$ as the solution to our minimization problem,
%\begin{equation}
%	\bvsi^{\star},\Theta\opt,\bvxi\opt = \min_{\bvsi,\Theta,\bvxi} \lag(\bvsi,\Theta,\bvxi,\bvmu)~\text{s.t.}~\xi_{z}(\sigma_{z} - \mu_{z}) = 0~\forall~z,~\bvmu\geq0
%\end{equation}
%
%\subsection{Minimization on the variances}
%Taking the derivative with respect to any of the variances yields
%\begin{equation}
%%	\label{eq:partial_derivative_var*z}
%	\pdv{\lag}{\var*{z}} = 2\sum_{\bvj} \bts{\real{\hat{R}_{\bvy;\bvj} - R_{\bvy;\bvj}}\real{R'_{\bvz;\bvj}} + \imag{\hat{R}_{\bvy;\bvj} - R_{\bvy;\bvj}}\imag{R'_{\bvz;\bvj}}} - \xi_{z}
%\end{equation}
%where $\xi_{z}$ is the Lagrange multiplier corresponding to $\var*{z}$. We also take the derivative with respect to $\xi_{z}$, which gives
%\begin{equation}
%%	\label{eq:partial_derivative_xi-z}
%	\pdv{\lag}{\xi_{z}} = \var*{z} - \mu_{z}
%\end{equation}
%With this, our minimization problem becomes a system of equations, where our first four conditions are
%\begin{equation}
%	\sum_{\bvj} \bts{\real{\hat{R}_{\bvy;\bvj} - R_{\bvy;\bvj}}\real{R'_{\bvz;\bvj}} + \imag{\hat{R}_{\bvy;\bvj} - R_{\bvy;\bvj}}\imag{R'_{\bvz;\bvj}}} - \xi_{z} = 0
%\end{equation}
%and our last four are given by \cref{eq:partial_derivative_xi-z}. From this we get
%\begin{equation}
%	A_{\bvx;\bvz} \var*{x} + A_{\bvu;\bvz} \var*{u} + A_{\bvga;\bvz} \var*{\gamma} + A_{\bvv;\bvz} \var*{v} = A_{\bvy;\bvz} + \xi_{z}
%\end{equation}
%in which
%\begin{equations}
%	A_{\bvw;\bvz}
%	& = \sum_{\bvj} \real{R'_{\bvw;\bvj}} \real{R'_{\bvz;\bvj}} + \imag{R'_{\bvw;\bvj}} \imag{R'_{\bvz;\bvj}} \\
%	& = \frac{1}{2} \sum_{\bvj} R_{\bvw;\bvj}' {R_{\bvz;\bvj}'}^* +{ R_{\bvw;\bvj}'}^* R_{\bvz;\bvj}'
%\end{equations}
%except for $A_{\bvy;\bvz}$, which uses the estimate correlation matrix $R_{\bvy;\bvj}$. We have four desired variables that compose $\bvsi$, and our four slack variables composing $\bvmu$, which are the unknowns of our system of equations.
%
%We form a system of equations of the form
%\begin{equation}
%	\bvXi \bvp = \bvq
%\end{equation}
%with
%\begin{subgather}
%	\bvA = \begin{bmatrix}
%		A_{\bvx;\bvx} & A_{\bvu;\bvx} & A_{\bvga;\bvx} & A_{\bvv;\bvx} \\
%		A_{\bvx;\bvu} & A_{\bvu;\bvu} & A_{\bvga;\bvu} & A_{\bvv;\bvu} \\
%		A_{\bvx;\bvga} & A_{\bvu;\bvga} & A_{\bvga;\bvga} & A_{\bvv;\bvga} \\
%		A_{\bvx;\bvv} & A_{\bvu;\bvv} & A_{\bvga;\bvv} & A_{\bvv;\bvv}
%	\end{bmatrix} \\
%	\bvXi = \begin{bmatrix}
%		\bvA & \bv{0}\SubSize{4}{4} \\
%		\Id\SubSize{4}{4} & -\Id\SubSize{4}{4}
%	\end{bmatrix} \\
%	\bvp = \begin{bmatrix}
%		\bvsi \\ \bvmu
%	\end{bmatrix} \\
%	\bvq = \tr{\begin{bmatrix}
%			A_{\bvy;\bvx} + \xi_{x} & A_{\bvy;\bvu} + \xi_{u} & A_{\bvy;\bvga} + \xi_{\gamma} & A_{\bvy;\bvv} + \xi_{v} & 0 & 0 & 0 & 0
%	\end{bmatrix}}
%\end{subgather}
%whose solution is
%\begin{equation}
%	\label{eq:solution_minimization_problem_variances}
%	\bvp[h] = \inv{\bvXi} \bvq
%\end{equation}
%However, since the last $4$ equations only state that $\mu_{z} = \var*{z}$, they aren't necessary for the solution over $\bvsi$, and this can be obtained through
%\begin{equation}
%	\bvsi[h] = \inv{\bvA} \bvq'
%\end{equation}
%with $\bvq'$ being the first $4$ elements of $\bvq$.
%
%Note that, from our Lagrangian problem, for each variable there are two options: either $\xi_{z} = 0$, meaning the constraint is inactive; or $\mu_{z} = 0$, implying that the constraint is active and therefore $\var*{z} = 0$ given that at our minima point we have that $\pdv{\lag}{\xi_{z}} = 0$.
%
%From here, there are two ways to find the most optimal solution to the problem: the first is to test every combination of the constraints being active or inactive, see which produces valid results ($\var*{z} \geq 0~\forall~z$), and from the valid ones which has the minimum value for our cost function. Note that when the $z$-th constraint is active we don't know $\xi_{z}$, but in this scenario $\mu_{z} = \var*{z} = 0$, and thus these aren't variables in the problem. With this, we can ignore their respective rows and columns in the system of equations, considering only the inactive constraints.
%
%The second option is the following: we start assuming that all constraints are inactive, meaning $\bvxi = \bv{0}$. We find $\bvp[h]$ for this case, and if any element of $\bvp[h]$ is negative, we set it to $0$ (as well as its slack variable) and repeat the process with this constraint active, until $\bvp[h] \geq \bv{0}$. This procedure isn't guaranteed to converge to the global minimum of the problem, though, since we don't verify every possible combination of activeness. Also, given that there're only $2^4 = 16$ combinations of activeness, it is a small time tradeoff between both of them.
%
%There also is a third option, which is a middleground of the two previous ones. We first test the unconstrained minimization, and see if any of its roots fall inside the desired constrained region. If so, that is our minimum point. If there are no minima point that fulfill the constraints, we fall back to the first case. This follows the logic that, if there are minimum points on the edge of the constraint region (in our case, a variance being zero), these would naturally appear on the unconstrained minimization solution.
%
%Also note that this minimization was done only with respect to $\bvsi$, ignoring $\Theta$. As any element of $\bvA$ that corresponds to $\bvu$ is dependent on $\Theta$, this solution depends on the angles as well. That is, for any directions $(\theta,\phi)$, the solution of \cref{eq:solution_minimization_problem_variances} will minimize the error between $\Corr*{\bvy}$ and $\Corr{\bvy}$. Thus, it is still necessary to minimize $J\pts{\Corr*{\bvy}(\bvsi)}$ with respect to the DoA.

%\subsection{Minimization on DoA}
%By definition, $R'_{\bvu;\bvj}(\theta,\phi)$ is
%\begin{equation}
%	R'_{\bvu;\bvj}(\theta,\phi) = e^{-\j \frac{2\pi f}{c} r_{\bvj} \cos\pts{\theta - \psi_{\bvj}} \cos\pts{\phi - \lambda_{\bvj}}}
%\end{equation}
%where $r_{\bvj}$, $\psi_{\bvj}$ and $\lambda_{\bvj}$ are the distance, azimuth, and elevation between the $i$th and $j$-th sensors. The azimuth is measured with respect to the $\x$-axis, ranging from $0$ to $2\pi$; and the elevation with respect to the $\x\text{-}\y$-plane (instead of the usual, which starts on the $\z$-axis and measures ``downwards''), ranging from $-\nfrac{\pi}{2}$ to $\nfrac{\pi}{2}$.
%
%\begin{figure}[h]
%	\centering
%	\import{figures/}{spherical_coordinates.pdf_tex}
%	\caption{Spherical coordinates system setup.}
%	\label{fig:spherical_coordinates_setup}
%\end{figure}
%
%Taking the Lagrangian's derivative with respect to $\theta$ yields
%\begin{subgather}
%	\pdv{\lag}{\theta} = 2\var*{u}\sum_{\bvj} \pts{\real{\hat{R}_{\bvy;\bvj} - R_{\bvy;\bvj}} \pdv{\real{R'_{\bvu;\bvj}}}{\theta} + \imag{\hat{R}_{\bvy;\bvj} - R_{\bvy;\bvj}} \pdv{\imag{R'_{\bvu;\bvj}}}{\theta}} \\
%	\pdv{\real{R'_{\bvu;\bvj}}}{\theta} = \frac{2\pi f}{c} r_{\bvj} \cos\pts{\phi - \lambda_{\bvj}} \sin\pts{\frac{2\pi f}{c} r_{\bvj} \cos\pts{\theta - \psi_{\bvj}} \cos\pts{\phi - \lambda_{\bvj}}} \sin\pts{\theta - \psi_{\bvj}} \\
%	\pdv{\imag{R'_{\bvu;\bvj}}}{\theta} = - \frac{2\pi f}{c} r_{\bvj} \cos\pts{\phi - \lambda_{\bvj}} \cos\pts{\frac{2\pi f}{c} r_{\bvj} \cos\pts{\theta - \psi_{\bvj}} \cos\pts{\phi - \lambda_{\bvj}}} \sin\pts{\theta - \psi_{\bvj}}
%\end{subgather}
%
%Through a similar process, the derivative with respect to $\phi$ is
%\begin{subgather}
%	\pdv{\lag}{\phi} = 2\var*{u}\sum_{\bvj} \pts{\real{\hat{R}_{\bvy;\bvj} - R_{\bvy;\bvj}} \pdv{\real{R'_{\bvu;\bvj}}}{\phi} + \imag{\hat{R}_{\bvy;\bvj} - R_{\bvy;\bvj}} \pdv{\imag{R'_{\bvu;\bvj}}}{\phi}} \\
%	\pdv{\real{R'_{\bvu;\bvj}}}{\phi} = \frac{2\pi f}{c} r_{\bvj} \cos\pts{\theta - \psi_{\bvj}} \sin\pts{\frac{2\pi f}{c} r_{\bvj} \cos\pts{\theta - \psi_{\bvj}} \cos\pts{\phi - \lambda_{\bvj}}} \sin\pts{\phi - \lambda_{\bvj}} \\
%	\pdv{\imag{R'_{\bvu;\bvj}}}{\phi} = -\frac{2\pi f}{c} r_{\bvj} \cos\pts{\theta - \psi_{\bvj}} \cos\pts{\frac{2\pi f}{c} r_{\bvj} \cos\pts{\theta - \psi_{\bvj}} \cos\pts{\phi - \lambda_{\bvj}}} \sin\pts{\phi - \lambda_{\bvj}}
%\end{subgather}
%
%Alternatively, these can be written as
%\begin{subgather}
%	\pdv{\lag}{\theta} = \alpha \sum_{\bvj} f_1(\theta,\phi,\bvj) g(\theta,\phi,\bvj) \\
%	\pdv{\lag}{\phi}   = \alpha \sum_{\bvj} f_2(\theta,\phi,\bvj) g(\theta,\phi,\bvj) \\[0.5cm]
%	\alpha = \frac{4\pi f\var*{u}}{c} \\
%	f_1(\theta,\phi,\bvj) = \sin\pts{\theta - \psi_{\bvj}} \cos\pts{\phi - \lambda_{\bvj}} \\
%	f_2(\theta,\phi,\bvj) = \cos\pts{\theta - \psi_{\bvj}} \sin\pts{\phi - \lambda_{\bvj}} \\
%	\begin{split}
%		g(\theta,\phi,\bvj)
%		= r_{\bvj} & \left[\real{\hat{R}_{\bvy;\bvj} - R_{\bvy;\bvj}} \sin\pts{\frac{2\pi f}{c} r_{\bvj} \cos\pts{\theta - \psi_{\bvj}} \cos\pts{\phi - \lambda_{\bvj}}} \right.\\
%		& - \left. 		   \imag{\hat{R}_{\bvy;\bvj} - R_{\bvy;\bvj}} \cos\pts{\frac{2\pi f}{c} r_{\bvj} \cos\pts{\theta - \psi_{\bvj}} \cos\pts{\phi - \lambda_{\bvj}}} \right]
%	\end{split}
%\end{subgather}
%
%One important factor is that $\var*{u} = 0$ implies that both $\pdv*{\lag}{\theta}$ and $\pdv*{\lag}{\phi}$ are zero. Although this is a solution to the problem of finding critical points to the function, they don't bring any information regarding the direction of the sources. Therefore, when calculating these derivatives for the iterative process, $\alpha$ will be treated as a constant and grouped within the step size of the gradient descent.

%\subsection{Considerations for different array configurations}
%
%If working with an uniform linear array, $\psi_{\bvj}$ and $\lambda_{\bvj}$ are constant across all sensors. Assuming that the ULA is positioned along the $\x$-axis, then $\psi_{\bvj} = 0$ and $\lambda_{\bvj} = 0$ for all $\bvj$, and $f_1(\cdot)$ and $f_2(\cdot)$ aren't dependent on $\bvj$. It is easy to see that we have trivial nulls at $\Theta = (\pm \nfrac{\pi}{2},\pm\nfrac{\pi}{2})$ and $\Theta = (0, 0)$, scenarios in which $f_1(\theta,\phi,\bvj) = f_2(\theta,\phi,\bvj) = 0$, and we have a null derivative. However, this corresponds to a maxima with respect to the angles, and as long as one doesn't initialize the minimization on this point, this problem can be disregarded.
%
%Given a direction $(\theta,\phi)$, there exists a direction $(\theta',0)$ such that their steering vectors are identical. That is,
%\begin{equation}
%	\cos(\theta) \cos(\phi) = \cos(\theta') \cos(0)
%\end{equation}
%and therefore
%\begin{equation}
%	\theta' = \arccos(\cos(\theta) \cos(\phi))
%\end{equation}
%
%We will call $\theta'$ the ``aperture'' of the original direction $\Theta$.
%
%This is readily noticeable if one notices the rotational symmetry of an ULA, which results in a cylindrical aliasing effect. With these considerations, the minimization problem can be reduced to a single angle (given that $\phi$ is assumed $0$) in the case of the ULA. If $\phi = 0$, $\cos(\phi)$ is maximized, therefore $f_1(\theta,\phi,\bvj)$ converges the fastest when minimizing with respect to $\theta$.
%
%\subsection{Minimization procedure}
%
%Our procedure, then, is as follows:
%\begin{enumerate}
%	\item Get an initial estimate for $\Theta$. This can be done by a random guess, or using a direction from a previous time.
%	\item Compute all combinations of $\bvsi$ for the different active/inactive conditions, and choose that which minimizes the cost function. Note that if the unconstrained minimization results in a valid vector of variances, the other combinations don't need to be tested.
%	\item Check if derivatives with respect to $\theta$ and $\phi$ are within a desired threshold:
%	\begin{enumerate}
%		\item If not, go back to step 2, with updated $\theta$ and $\phi$ using a gradient descent method;
%		\item If yes, break, and return the estimated parameters.
%	\end{enumerate}
%	\item Construct the NCM $\Corr{\bveta}$ as in \cref{eq:def_NCM_Corr-bveta}.
%\end{enumerate}
%
%\subsection{Cross-frequency direction estimation}
%
%Although the variances for each $\bvsi$ can radically change for different frequencies, the desired DoA doesn't. Therefore, in an ideal scenario, the calculated DoA $\Theta[l,k]$, which is frequency-dependent, is actually constant across frequency.
%
%With this, given the frequency-dependent estimate DoA $\Theta[l,k]$, we can estimate a direction $\bar{\Theta}[l]$ which is frequency-independent. The most appropriate way is to average the angular phasors for $\theta$ and $\phi$. That is, $U_{\theta} = e^{\j\theta}$, and $U_{\phi} = e^{\j\phi}$. We then have that
%\begin{subgather}
%	\bar{\theta}[l] = \angle {\sum_{k} h(l,k) e^{\j\theta[l,k]}} \\
%	\bar{\phi}[l] = \angle {\sum_{k} h(l,k) e^{\j\phi[l,k]}}
%\end{subgather}
%with $h(l,k)$ being a weighing function. Three sensible options are $h(l,k) = 1$, $h(l,k) = \std*{u}[l,k]$, and $h(l,k) = \var*{u}[l,k]$, each giving (increasingly) more significance to frequency bins with more undesired signal power.
%
%Averaging the angles directly isn't correct, as there would be undesired effects on the boundaries of the angular sets. Assuming that one estimate resulted in $\theta = 1\dg$ and another in $\theta = 359\dg$, the na√Øve average would be $180\dg$, instead of the more sensible $0\dg$.
%
%\subsection{Source-tracking procedure}
%
%Notice that in the minimization procedure, an initial estimate for the unknown source's DoA is required. Although possibly initially a multi-initialization process may be necessary, the source's DoA won't change rapidly, thus lying within a region-of-convergence around the previous window's DoA. Formally, we can use the previous estimate $\bar{\Theta}[l-1]$ as the initial guess for the current window $\Theta_0[l,k]$ on all frequencies. If the source is rapidly moving, a multi-initialization is necessary, but possibly only around $\bar{\Theta}[l-1]$, instead of covering all the polar coordinate system.